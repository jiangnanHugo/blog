I"†<!--
### Model setups

- Encoder: $$y=Wx$$
- Decoder: $$\hat x=Uy$$
- Loss:
$$\min \ell=||x-\hat x||^2$$

### Usage

- Dimension reduction
 -->

<h4 id="1-kl-divergence">1. KL Divergence</h4>
<p>\(\text{KL}(p||q)\) means the information of \(q(x)\) minus the information of \(p(x)\), so it should be like:</p>

\[\text{KL}(p||q)=-\sum q(x)\log q(x) +\sum p(x) \log p(x)\]

<p>But we should bear in mind that ‚Äúwith respect to distribution \(p(x)\)‚Äù. So the exact definition should change the first term \(\sum q(x)\log q(x)\) to  \(\sum p(x)\log q(x)\). So the \(\text{KL}(p\|q)\) would be:</p>

\[\begin{aligned}
\text{KL}(p||q)&amp;=-\sum p(x)\log q(x) +\sum p(x) \log p(x) \\
&amp;=\sum p(x)\log \frac{p(x)}{q(x)} \\
&amp;=-\sum p(x)\log \frac{q(x)}{p(x)}
\end{aligned}\]

<p>and its traits: 1) \(\text{KL}\ge 0\); 2)
\(\text{KL}(p||q)\neq \text{KL}(q||p)\). So it‚Äôs not distance but divergence, since distance should be symmetric.</p>

<h4 id="2-graphical-modeling">2. Graphical Modeling</h4>
<p>Variational technique is mostly used in Graphical model. So suppose there is \(z,x\), \(z\) is my hidden variables and \(x\) is my observation. The posterior probability:</p>

\[p(z|x)=\frac{p(x|z)p(z)}{p(x)}=\frac{p(x,z)}{p(x)}\]

<p>computing \(p(x)\) is complicated, we need to calculate the marginal distribution, which is:</p>

\[p(x)=\int p(x|z)p(z)dz\]

<p>this integral is intractable in many cases. So roughly there are two main approaches to handle this: 1). Monte Carlo sampling; 2) Variational Inference. The first approach is zero bias with high variance, while the second one has zero variance but is biased estimator.</p>

<h4 id="3-variational-inference">3. Variational Inference</h4>

<p>approximate
\(p(z|x)\)
by another distribution \(q(z)\), which suppose to be a attractable distribution. So we want to minimize the joint loss</p>

<!-- $$\\
\-->
<p>\(\begin{aligned}
\min \text{KL}(q(z)\|p(z|x))&amp;=-\sum q(z)\log \frac{p(z|x)}{q(z)} \\
&amp;=-\sum q(z)\log \frac{p(x,z)}{p(x)}\times \frac{1}{q(z)}\\
&amp;=-\sum q(z)\log \frac{p(x,z)}{q(z)} \times \frac{1}{p(x)}\\
&amp;=-\sum q(z)\log \frac{p(x,z)}{q(z)}-\log p(x)\\
&amp;=-\sum q(z)\log \frac{p(x,z)}{q(z)}+\sum_z q(z)\log p(x)\end{aligned}\)
Since \(\sum_z q(z)\log p(x)=\log p(x)\sum_z q(z) =\log p(x)\) , so we can transform the equation into this form:</p>

\[\log p(x)=\text{KL}(q(x)\|p(x|x)+\sum q(z)\log \frac{p(x,z)}{q(z)}\]

<p>where \(\log p(x)\) is a constant. The \(\text{KL}\) is the term we want to minimize in the first place, so that we can maximize the second term, which is called varitional lower bound.</p>

\[\ell=\sum q(z)\log \frac{p(x,z)}{q(z)}\]

<p>as \(\ell \leq \log p(x)\)</p>

<p>If we maximize the lower bound of this function, we are also maximize the original function.</p>

<h4 id="references">References</h4>
<ul>
  <li><a href="https://www.youtube.com/watch?v=uaaqyVS9-rM">Ali Ghodsi, Lec : Deep Learning, Variational Autoencoder, Oct 12 2017</a></li>
</ul>
:ET