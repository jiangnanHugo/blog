I"<h3 id="one-sample">one sample:</h3>

\[x_i \to [y_i^0,\cdots,y_{i}^{k}]\]

<p>where \(y_i^0\) are true labeled words , and \(y_i^1,\cdots,y_i^{k}\) are noise samples word index, which is generated by unigram distribution \(q(w)\) of the dataset.</p>

<ol>
  <li>the probability of true data:</li>
</ol>

\[p(y_i^0=1|x_i,\theta)=\frac{\exp(y_i^0,h_\theta)}{\exp(y_i^0 h_\theta) + k*q(y_i^0)}\]

<ol>
  <li>the noise sample probability:</li>
</ol>

\[p(y_i^t=0|x_i,\theta)=\frac{k*q(y_i^t)}{\exp(y_i^t h_\theta) + k*q(y_i^t)},t=1,\cdots,k\]

<ol>
  <li>the cost function of this sample:</li>
</ol>

\[\ell_{nce}=\log p(y_i^0|x_i,\theta)+\sum_{t=1}^k{\log p(y_i^t|x_i,\theta)}\]

<ol>
  <li>the overall cost function of the dataset:</li>
</ol>

\[\ell_{nce}=\frac{1}{N}\sum_i^N{\left\{\log p(y_i^0|x_i,\theta)+\sum_{t=1}^k{\log p(y_i^t|x_i,\theta)}\right\}}\]

<h3 id="references">References</h3>
<ul>
  <li>Noise-Contrastive Estimation of Unnormalized Statistical Models with Applications to Natural Image Statistics</li>
  <li>Word2vec Parameter Learning Explained</li>
  <li>Efficient Estimation of Word Representation in Vector Space</li>
  <li>Distributed Representations of Words and Phrases and their Compositionality</li>
  <li>Notes on Noise Contrastive Estimation and Negative Sampling</li>
</ul>
:ET