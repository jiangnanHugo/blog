I"e<p>Given a list of word vectors: \(x_1,\cdots,x_{t-1},x_t,x_{t+1},\cdots,x_T\)</p>

\[\begin{array}{l}
\hat y_t &amp;=\text{softmax}(W h_t) \\
\hat p(x_{t+1}=v_j|x_t,\cdots,x_1) &amp;=\hat{y}_{t,j}
\end{array}\]

<p>\(h_t\) is the hidden state output at time step \(t\). \(\hat{y}\in\mathbb{R}^{V}\) is a probability distribution over the vocabulary, same cross entropy loss function but predicting words instead of classes</p>

\[J^{(t)}(\theta)=-\sum_{j=1}^{|V|}{y_{t,j}\log \hat{y}_{t,j}}\]

<p>Evaluation could just be negative of average log probability over dataset of size (number of words) T:</p>

\[\ell=-\frac{1}{T}\sum_{t=1}^T\sum_{j=1}^{|V|}{y_{t,j}\log \hat{y}_{t,j}}\]

<p>Perplexity: \(\exp(\ell)\)</p>

<p>Lower is better !</p>

<h3 id="references">References</h3>
<ul>
  <li>
    <p><a href="http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture8.pdf">cs224n: Natural Language Processing with Deep Learning - Lecture 7</a></p>
  </li>
  <li>
    <p><a href="https://courses.engr.illinois.edu/cs498jh/Slides/Lecture04.pdf">CS 498: Introduction to Natural Language Processing</a></p>
  </li>
</ul>
:ET