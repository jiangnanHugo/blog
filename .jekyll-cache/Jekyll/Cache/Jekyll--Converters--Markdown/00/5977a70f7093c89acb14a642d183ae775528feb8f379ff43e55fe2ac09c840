I"+<h4 id="1-the-l1-norm">1. The L1-norm</h4>
<p>Use absolute error:</p>

\[\arg\min_{w\in \mathbb{R}^d}\sum_{i=1}{n}{|w^\top x_i -y_i|}\]

<p>as the target function. Now the model can put less focus on far-away points, which is a form of <strong>Robust Regression</strong>.</p>

<p>If we write this as matrix form, which is L1-norm:</p>

\[\arg\min_{w\in \mathbb{R}^d}{\|x_w -y\|}_{1}\]

<p>where \(x_w=[w^\top x_0,w^\top x_1,\cdots, ]\).</p>

<h4 id="2-the-non-smooth-issue">2. The non-smooth issue</h4>
<p>The gradient does not exist at \(x=0\) points, such that it is not a convex function at \(x=0\) point and makes the absolute error minimization harder. One method for this case is to transform this absolute error as <strong>linear programming</strong>.</p>

<h4 id="3-the-constrained-problems">3. The constrained problems</h4>
<p>We can convert the L1-norm as the maximum of smooth functions:</p>

\[\|w\|\to \max\{w_l -w\}\]

<ol>
  <li>replace <strong>maximum with new variable</strong>, constrained to upper-bound max:</li>
</ol>

\[\|w\| \to \arg\min_{w\in R_l, r\in R}{r_i}, r\ge \max{w_i-w}\]

<p>where the residual variable \(r=\|w\|\)</p>

<ol>
  <li>replace individual constraint with <strong>constraint for each element</strong> of max:</li>
</ol>

\[\|w\| \to \arg\min_{w\in R_l, r\in R}{r_i}\]

\[\text{s.t.}\prod r\ge w\]

\[r\ge -w\]

<h4 id="references">References</h4>
<ul>
  <li><a href="http://video.impa.br/index.php?page=mini-courses-svan-2016">Mini Course 3 - Stochastic Convex Optimization Methods In Machine Learning</a></li>
  <li><a href="https://stats.stackexchange.com/questions/147001/is-minimizing-squared-error-equivalent-to-minimizing-absolute-error-why-squared">Is minimizing squared error equivalent to minimizing absolute error? Why squared error is more popular than the latter?</a></li>
</ul>
:ET