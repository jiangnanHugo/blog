I"²<p><img src="http://licstar.net/wp-content/uploads/2013/07/1.png" alt="LBL model" /></p>
<h3 id="the-model">The model</h3>
<ol>
  <li>the log-bilinear (LBL) model is perhaps the simplest neural language model.</li>
  <li>Given the previous context words \(\{w_1,\cdots,w_{n-1}\}\), the <strong>LBL</strong> model first predicts the representation for the next word \(w_n\) by linearly combining the representations of the context words:</li>
</ol>

\[\tilde r=\sum_{i=1}^{n-1}{C_i r_{w_i}}\]

<p>where, \(r_w\) is the real-valued vector representing word \(w\).</p>
<ol>
  <li>Then the distribution for the next word is computed based on the similarity between the predicted representation and the representations of all words in the vocabulary:</li>
</ol>

\[P(w_n=w|w_{1:n-1})=\frac{\exp(\tilde r^\top r_w)}{\sum_j{\exp(\tilde r^\top r_j)}}\]

<h3 id="why-it-is-called-bilinear-">why it is called <code class="language-plaintext highlighter-rouge">bilinear</code> ?</h3>
<ul>
  <li>\(\tilde r\) is the combination of the previous word embeddings</li>
  <li>\(r_w\) is the embedding of word $w$.</li>
  <li>the multiplication of them \(\tilde r^\top r_w\) is called <strong>bilinear map</strong>. As such, there only exists <strong>one</strong> embedding looking-up table.</li>
</ul>

<h3 id="bilinear-map">Bilinear map</h3>
<p>In mathematics, a <code class="language-plaintext highlighter-rouge">bilinear map</code> is a function combining elements of two vector spaces to yield an element of a third vector space, and is linear in each of its arguments. Matrix multiplication is an example.
A bilinear function (see https://en.wikipedia.org/wiki/Bilinear_map ) is a function which is linear in each variable when all other variables are fixed. For instance:</p>

\[f(x,y) = x * y\]

\[\log f(x,y)=\log x+\log y\]

<h3 id="references">References</h3>
<ul>
  <li><a href="https://www.quora.com/What-is-a-log-bilinear-model">What is a log-bilinear model?</a></li>
</ul>
:ET