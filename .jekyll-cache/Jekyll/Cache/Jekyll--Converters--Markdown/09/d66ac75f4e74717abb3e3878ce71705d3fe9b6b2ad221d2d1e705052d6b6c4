I"Á<h4 id="1-hierarchical-clustering-approach">1. Hierarchical Clustering Approach</h4>

<ul>
  <li>A typical clustering analysis approach via partitioning data set sequentially</li>
  <li>Construct nested partitions layer by layer via grouping objects into a tree of clusters (without the need to know the number of clusters in advance)</li>
  <li>Use (generalized) distance matrix as clustering criteria.</li>
</ul>

<h4 id="2-strategiesagglomerative-vs-divisive">2. Strategiesï¼šAgglomerative Vs. Divisive</h4>

<p>Two sequential clustering strategies for constructing a tree of clusters</p>

<ul>
  <li>Agglomerative: a bottom-up strategy
    <ul>
      <li>Initially each data object is in its own (atomic) cluster</li>
      <li>Then merge these atomic clusters into larger and larger clusters</li>
    </ul>
  </li>
  <li>Divisive: a top-down strategy
    <ul>
      <li>Initially all objects are in one single cluster</li>
      <li>Then the cluster is subdivided into smaller and smaller clusters</li>
    </ul>
  </li>
</ul>

<h4 id="3-hierarchical-clustering">3. Hierarchical Clustering</h4>

<p>The number of dendrograms with n: \(\text{leafs}=\frac{(2n-3)!}{[2^{(n-2)}(n-2)!]}\)</p>

<p>Bottom-Up (agglomerative): Starting with each item in its own cluster, find the best pair to merge into a new cluster. Repeat until all clusters are fused together.</p>

<h4 id="4-compute-distances-between-clusters">4. Compute distances between clusters</h4>

<ul>
  <li><strong>Single Link</strong>: smallest distance between an element in one cluster and an element in the other. e.g.,</li>
</ul>

\[d(c_i,c_j)=\min(d(x_{ip},d_{jq}))\]

<ul>
  <li><strong>Complete Link</strong>: largest distance between an element in one cluster and an element in the other, i.e.,</li>
</ul>

\[d(c_i,c_j)=\max(d(x_{ip},d_{jq}))\]

<ul>
  <li><strong>Average</strong>: avg distance between elements in one cluster and elements in the other, i.e.,</li>
</ul>

\[d(c_i,c_j)=avg\{d(x_{ip},d_{jq})\}\]

<h4 id="5-summary">5. Summary</h4>

<p>hierarchical algorithm is a sequential clustering algorithm</p>
<ul>
  <li>use distance matrix to construct a tree of clusters (dendrogram)</li>
  <li>hierarchical representation without the need of knowing # of clusters</li>
</ul>

<p>major weakness of agglomerative clustering methods</p>
<ul>
  <li>can never undo what was done previously</li>
  <li>sensitive to cluster distance measures and noise/outliers</li>
  <li>less efficient: \(O(n^2\log n)\), where n is the number of total objects</li>
</ul>

<h4 id="6-comparison-with-k-means">6. Comparison with k-means</h4>

<ul>
  <li>kmeans clustering produces a single partitioning</li>
  <li>hierarchical clustering can give different partitionings depending on the level-of-resolution we are looking at</li>
  <li>kmeans clustering needs the number of clusters to be specified</li>
  <li>hierarchical clustering doesnâ€™t need the number of clusters to be specified</li>
  <li>kmeans clustering is usually more efficient run-time wise</li>
  <li>hierarchical clustering can be slow (has to make several merge/split decisions)</li>
  <li>no clear consensus on which of the two produces better clustering</li>
</ul>

<h4 id="references">References</h4>
<ul>
  <li><a href="https://www.youtube.com/watch?v=OGxgnH8y2NM&amp;list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v">Practical Machine Learning with Python</a></li>
  <li><a href="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html">Hierarchical clustering</a></li>
</ul>
:ET